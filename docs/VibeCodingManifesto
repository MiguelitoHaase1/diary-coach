# Vibecoding manifesto

##Philosophy: Three Key Principles
I believe good practices from "human product team" still stand with GenAI... they can just get 10-100X faster with 10X smaller teams. Examples of good practices (TDD, Test and deploy frequently, thoughtful emerging architecture designed for anti-fragility, split up learn Vs execute,...), living up to these 3 principles:


### 1. Compartmentalization Through Incremental Development
**Minimize cognitive load and context window overflow** by breaking work into small, testable increments. Each session (defined in roadmap.md) is further decomposed into bite-sized tasks that can be independently tested and validated. This prevents AI context overflow and improves observability.

### 2. Continuous Improvement Through Evaluation
**Set up testable evaluations** to measure and improve AI collaboration effectiveness. Track what works, what doesn't, and refine the approach based on empirical results. Every session generates logs and status updates that feed back into better future interactions.

### 3. Learning While Building
**Transform delegation into education**. The AI doesn't just write codeâ€”it creates learning opportunities through Dojo documents that help you understand the "why" behind every technical decision. Building becomes teaching.

##Workflow for a product
1 - Write a vision with objectives and JTBDs plus a quick prototype 
2 - Define a composable architecture that starts hyper-testable (e.g., CLI front-end)... here always consider at least two very distinct approaches. 
3 - build roadmap in these generic steps:
i) Test and environment setup
ii) Lean prototype that can actually evolve (e.g., in CLI)
iii) Setup evals, and ensure Evals are measured in every test and when I try it
iv) Evolve product, and follow evals... they HAVE to increase
4: Once start any session, start creating a more detailed session_x.md to describe approach and break-into increments

## Operational choices
1: **Claude.md**: TDD, build in small simple testable increments (never move on wo tests passing), systematic logging, code quality, 
2: Every increment, create **logbooks** for traceability and **dojos** for learning
3: End of every session, update **status.md** (and push to git and run tests with eval)
